{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 19:15:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import imageio\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "\n",
    "min_date = '2022-04-01 12:00:00'\n",
    "\n",
    "color_name = {\n",
    "    16729344: 'red', \n",
    "    0: 'black', \n",
    "    6970623: 'periwinkle', \n",
    "    7143450: 'burgandy', \n",
    "    16775352: 'pale yellow', \n",
    "    2379940: 'dark blue', \n",
    "    8461983: 'dark purple', \n",
    "    16754688: 'orange', \n",
    "    40618: 'teal', \n",
    "    52416: 'light teal', \n",
    "    8318294: 'light green', \n",
    "    12451897: 'dark red', \n",
    "    16757872: 'beige', \n",
    "    9745407: 'lavendar', \n",
    "    16777215: 'white', \n",
    "    11815616: 'purple', \n",
    "    41832: 'dark green', \n",
    "    10250534: 'brown', \n",
    "    14986239: 'pale purple', \n",
    "    16766517: 'yellow', \n",
    "    3576042: 'blue', \n",
    "    16751018: 'light pink', \n",
    "    5368308: 'light blue', \n",
    "    9014672: 'gray', \n",
    "    4799169: 'indigo', \n",
    "    30063: 'dark teal', \n",
    "    13948889: 'light gray', \n",
    "    5329490: 'dark gray', \n",
    "    52344: 'green', \n",
    "    16726145: 'pink', \n",
    "    7161903: 'dark brown', \n",
    "    14553215: 'magenta'\n",
    "}\n",
    "\n",
    "db_path = r\"../data/lossless_2022_place_history.parquet\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.load(db_path, format=\"parquet\", pathGlobFilter=\"*.parquet\")\n",
    "df.repartition(\"user_id\")\n",
    "df.createOrReplaceTempView('data')\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def time_delta(curr_date : str):\n",
    "    min_tokens = min_date.split(' ')\n",
    "    curr_tokens = curr_date.split(' ')\n",
    "    curr_time = curr_tokens[1].split(':')\n",
    "    seconds_diff = float(curr_time[2])\n",
    "    minute_diff = 60 * int((curr_time)[1]) - int((min_tokens[1].split(':'))[1])\n",
    "    hour_diff = 60 * 60 * (int((curr_tokens[1].split(':'))[0]) - int((min_tokens[1].split(':'))[0]))\n",
    "    day_diff = 24 * 60 * 60 * (int((curr_tokens[0].split('-'))[2]) - int((min_tokens[0].split('-'))[2]))\n",
    "    time_diff = seconds_diff + minute_diff + hour_diff + day_diff\n",
    "    return time_diff\n",
    "\n",
    "# Returns rgb values in a list from an integer representation of hex code\n",
    "def int_to_rgb(color_value : int) -> list[int]:\n",
    "    hex_code = f\"{color_value:06x}\" # Convert To Hex\n",
    "    return [ int(hex_code[0:2], 16), int(hex_code[2:4], 16), int(hex_code[4:6], 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis functions\n",
    "# Create most recent img from two coordinates\n",
    "def draw_curr_img(top_left_x : int, top_left_y : int, bottom_right_x : int, bottom_right_y : int, time : float):\n",
    "    x_range = bottom_right_x - top_left_x + 1\n",
    "    y_range = bottom_right_y - top_left_y + 1\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{top_left_x} AS x_img, y-{top_left_y} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND x >= {top_left_x} AND x <= {bottom_right_x}\n",
    "        AND y >= {top_left_y} AND y <= {bottom_right_y}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    img_set = np.full([y_range, x_range, 3], 50, dtype=np.uint8) # fill with dark grey to show black\n",
    "    for coord in coords:\n",
    "        img_set[coord[1]][coord[0]] = int_to_rgb(coord[2])\n",
    "\n",
    "    img = Image.fromarray(img_set, \"RGB\")\n",
    "    file_name = f\"{top_left_x}_{top_left_y}_{bottom_right_x}_{bottom_right_y}_{time}.png\"\n",
    "    img.save(file_name)\n",
    "    shutil.move(file_name, f\"visualizations/pngs/{file_name}\")\n",
    "    img_set = None\n",
    "\n",
    "# Create most recent img from two coordinates from a given user\n",
    "def draw_curr_img_from_user(top_left_x : int, top_left_y : int, bottom_right_x : int, bottom_right_y : int, user_id : int, time : float):\n",
    "    x_range = bottom_right_x - top_left_x + 1\n",
    "    y_range = bottom_right_y - top_left_y + 1\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{top_left_x} AS x_img, y-{top_left_y} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND user_id = {user_id}\n",
    "        AND x >= {top_left_x} AND x <= {bottom_right_x}\n",
    "        AND y >= {top_left_y} AND y <= {bottom_right_y}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    img_set = np.full([y_range, x_range, 4], 0, dtype=np.uint8) # fill with dark grey to show black\n",
    "    for coord in coords:\n",
    "        rgba = int_to_rgb(coord[2])\n",
    "        rgba.append(255)\n",
    "        img_set[coord[1]][coord[0]] = rgba\n",
    "\n",
    "    img = Image.fromarray(img_set, \"RGBA\")\n",
    "    file_name = f\"{user_id}_{top_left_x}_{top_left_y}_{bottom_right_x}_{bottom_right_y}_{time}.png\"\n",
    "    img.save(file_name)\n",
    "    shutil.move(file_name, f\"visualizations/pngs/{file_name}\")\n",
    "    img_set = None\n",
    "\n",
    "# Create most recent img from two coordinates focused on a given user\n",
    "def draw_curr_img_focus_users(top_left_x : int, top_left_y : int, bottom_right_x : int, bottom_right_y : int, user_id : list[int], time : float):\n",
    "    x_range = bottom_right_x - top_left_x + 1\n",
    "    y_range = bottom_right_y - top_left_y + 1\n",
    "    user_id_str = '('+', '.join(map(str, user_id))+')'\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{top_left_x} AS x_img, y-{top_left_y} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND user_id NOT IN {user_id_str}\n",
    "        AND x >= {top_left_x} AND x <= {bottom_right_x}\n",
    "        AND y >= {top_left_y} AND y <= {bottom_right_y}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    img_set = np.full([y_range, x_range, 4], 0, dtype=np.uint8) # fill with dark grey to show black\n",
    "    for coord in coords:\n",
    "        rgba = int_to_rgb(coord[2])\n",
    "        rgba.append(100)\n",
    "        img_set[coord[1]][coord[0]] = rgba\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{top_left_x} AS x_img, y-{top_left_y} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND user_id IN {user_id_str}\n",
    "        AND x >= {top_left_x} AND x <= {bottom_right_x}\n",
    "        AND y >= {top_left_y} AND y <= {bottom_right_y}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    for coord in coords:\n",
    "        rgba = int_to_rgb(coord[2])\n",
    "        rgba.append(255)\n",
    "        img_set[coord[1]][coord[0]] = rgba\n",
    "\n",
    "    img = Image.fromarray(img_set, \"RGBA\")\n",
    "    file_name = f\"{user_id}_focus_{top_left_x}_{top_left_y}_{bottom_right_x}_{bottom_right_y}_{time}.png\"\n",
    "    img.save(file_name)\n",
    "    shutil.move(file_name, f\"visualizations/pngs/{file_name}\")\n",
    "    img_set = None\n",
    "\n",
    "# Create most recent img from a block given its relative position and size\n",
    "def draw_curr_img_at_group(x_group : int, y_group: int, group_size : int, time : float):\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{x_group*group_size} AS x_img, y-{y_group*group_size} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND x >= {x_group*group_size} AND x < {(x_group+1)*group_size}\n",
    "        AND y >= {y_group*group_size} AND y < {(y_group+1)*group_size}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    img_set = np.full([group_size, group_size, 3], 50, dtype=np.uint8) # fill with dark grey to show black\n",
    "    for coord in coords:\n",
    "        img_set[coord[1]][coord[0]] = int_to_rgb(coord[2])\n",
    "\n",
    "    img = Image.fromarray(img_set, \"RGB\")\n",
    "    file_name = f\"{x_group}_{y_group}_{group_size}_{time}.png\"\n",
    "    img.save(file_name)\n",
    "    shutil.move(file_name, f\"visualizations/pngs/{file_name}\")\n",
    "    img_set = None\n",
    "\n",
    "# Draw user placements in a block given block position and size\n",
    "def draw_curr_img_at_group_from_users(x_group : int, y_group: int, group_size : int, user_id : list[int], time : float):\n",
    "    user_id_str = '('+', '.join(map(str, user_id))+')'\n",
    "\n",
    "    sql_to_execute = f\"\"\"\n",
    "        SELECT x-{x_group*group_size} AS x_img, y-{y_group*group_size} AS y_img, color FROM data\n",
    "        WHERE timedelta <= {time}\n",
    "        AND user_id IN {user_id_str}\n",
    "        AND x >= {x_group*group_size} AND x < {(x_group+1)*group_size}\n",
    "        AND y >= {y_group*group_size} AND y < {(y_group+1)*group_size}\n",
    "        ORDER BY timedelta ASC\n",
    "    \"\"\"\n",
    "\n",
    "    coords = spark.sql(sql_to_execute).collect()\n",
    "\n",
    "    img_set = np.full([group_size, group_size, 3], 50, dtype=np.uint8) # fill with dark grey to show black\n",
    "    for coord in coords:\n",
    "        img_set[coord[1]][coord[0]] = int_to_rgb(coord[2])\n",
    "\n",
    "    img = Image.fromarray(img_set, \"RGB\")\n",
    "    file_name = f\"{user_id}_{x_group}_{y_group}_{group_size}_{time}.png\"\n",
    "    img.save(file_name)\n",
    "    shutil.move(file_name, f\"visualizations/pngs/{file_name}\")\n",
    "    img_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = time_delta(\"2022-04-01 12:00:00\")\n",
    "max = time_delta(\"2022-04-01 18:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                        (0 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|actions|\n",
      "+-------+-------+\n",
      "|  78352|    795|\n",
      "| 122804|    781|\n",
      "| 280238|    777|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  SELECT user_id, COUNT(1) AS actions FROM data GROUP BY user_id ORDER BY actions DESC LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|user_id|         min(prev)|         avg(prev)|         max(prev)|               p50|              p90|               p99|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  78352|300.57399999999325| 376.3957279596977| 18960.30900000001|305.54449999999997|337.3666000000005| 878.5880799999782|\n",
      "| 122804|300.02899999998044|383.56283589743595|27719.151000000005|  304.833999999988|369.4231000000029|1309.8635000000254|\n",
      "| 280238| 300.1109999999753|382.65825386597936| 7073.528999999995| 317.6334999999963|404.1359999999995|1567.8967500000035|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH pixel_times AS (\n",
    "        SELECT user_id, timedelta, (timedelta-LAG(timedelta) OVER (PARTITION BY user_id ORDER BY timedelta)) AS prev FROM data WHERE user_id IN (78352, 122804, 280238)\n",
    "    )\n",
    "       \n",
    "    SELECT \n",
    "       user_id, MIN(prev), AVG(prev), MAX(prev),\n",
    "       PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY prev) AS p50,  \n",
    "       PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY prev) AS p90, \n",
    "       PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY prev) AS p99\n",
    "    FROM pixel_times GROUP BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11\n",
      "11\n",
      "12\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "6\n",
      "8\n",
      "10\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "0\n",
      "9\n",
      "12\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "5\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "10\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "10\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "10\n",
      "9\n",
      "2\n",
      "0\n",
      "11\n",
      "11\n",
      "9\n",
      "11\n",
      "10\n",
      "10\n",
      "9\n",
      "11\n",
      "6\n",
      "7\n",
      "10\n",
      "9\n",
      "12\n",
      "12\n",
      "11\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "11\n",
      "11\n",
      "12\n",
      "10\n",
      "11\n",
      "9\n",
      "8\n",
      "11\n",
      "6\n",
      "8\n",
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "10\n",
      "10\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "10\n",
      "8\n",
      "3\n",
      "12\n",
      "8\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "10\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "10\n",
      "7\n",
      "1\n",
      "0\n",
      "9\n",
      "11\n",
      "12\n",
      "9\n",
      "10\n",
      "8\n",
      "10\n",
      "11\n",
      "10\n",
      "11\n",
      "10\n",
      "12\n",
      "11\n",
      "10\n",
      "11\n",
      "11\n",
      "10\n",
      "11\n",
      "11\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "7\n",
      "10\n",
      "10\n",
      "8\n",
      "0\n",
      "3\n",
      "6\n",
      "6\n",
      "9\n",
      "2\n",
      "6\n",
      "6\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "8\n",
      "9\n",
      "9\n",
      "2\n",
      "3\n",
      "2\n",
      "8\n",
      "7\n",
      "12\n",
      "11\n",
      "8\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "5\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results = spark.sql(f\"\"\"\n",
    "    WITH pixel_times AS (\n",
    "        SELECT user_id, CAST(CAST(timedelta AS INTEGER)/3600 AS INTEGER) As hr FROM data WHERE user_id IN (78352, 122804, 280238)\n",
    "    ), hourly_pixels AS (\n",
    "        SELECT user_id, hr, COUNT(1) AS actions FROM pixel_times GROUP BY user_id, hr\n",
    "    )\n",
    "              \n",
    "    SELECT user_id, hr, actions FROM hourly_pixels ORDER BY user_id, hr\n",
    "\"\"\").collect()\n",
    "\n",
    "hourly_pixels = {\n",
    "    78352: [], \n",
    "    122804: [], \n",
    "    280238: []\n",
    "}\n",
    "for result in results:\n",
    "    hourly_pixels[result[0]].append((result[1], result[2]))\n",
    "\n",
    "for user in hourly_pixels:\n",
    "    actions = [0 for _ in range(85)]\n",
    "    for res in hourly_pixels[user]:\n",
    "        actions[res[0]] = res[1]\n",
    "    for act in actions:\n",
    "        print(act)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|user_id|x_group|y_group|actions|\n",
      "+-------+-------+-------+-------+\n",
      "| 280238|      7|      5|     60|\n",
      "| 280238|      7|      6|      4|\n",
      "| 280238|      8|      5|    672|\n",
      "| 280238|      8|      6|     25|\n",
      "| 280238|     24|      2|      1|\n",
      "| 280238|     25|      1|      1|\n",
      "| 280238|     26|     16|      7|\n",
      "| 280238|     27|     16|      3|\n",
      "| 280238|     33|     31|      4|\n",
      "+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 280238 User 3 Is Singled Out\n",
    "spark.sql(\"\"\"\n",
    "    WITH group_marker AS (\n",
    "        SELECT user_id, CAST(x/50 AS INTEGER) AS x_group, CAST(y/50 AS INTEGER) AS y_group, x, y FROM data WHERE user_id IN (280238)\n",
    "    ), grouped_placements AS (\n",
    "        SELECT user_id, x_group, y_group, COUNT(1) AS actions FROM group_marker GROUP BY user_id, x_group, y_group\n",
    "    )\n",
    "       \n",
    "    SELECT user_id, x_group, y_group, actions FROM grouped_placements WHERE user_id IN (280238) ORDER BY x_group, y_group, actions DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dark blue : 538\n",
      "yellow : 191\n",
      "black : 16\n",
      "orange : 13\n",
      "white : 13\n",
      "dark red : 4\n",
      "indigo : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "colors = spark.sql(\"\"\"\n",
    "    SELECT user_id, color, COUNT(1) AS amt FROM data WHERE user_id IN (280238) GROUP BY user_id, color ORDER BY amt DESC\n",
    "\"\"\").collect()\n",
    "\n",
    "for color in colors:\n",
    "    print(f\"{color_name[color[1]]} : {color[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|user_id|x_group|y_group|actions|\n",
      "+-------+-------+-------+-------+\n",
      "| 122804|     12|      5|     93|\n",
      "|  78352|     33|      5|     91|\n",
      "| 122804|     33|      4|     86|\n",
      "|  78352|     18|     37|     80|\n",
      "| 122804|     32|      4|     79|\n",
      "| 122804|     18|     37|     74|\n",
      "|  78352|     12|      4|     65|\n",
      "|  78352|     32|      5|     62|\n",
      "| 122804|     32|      5|     60|\n",
      "|  78352|     12|      5|     55|\n",
      "|  78352|     33|      4|     55|\n",
      "| 122804|     12|      4|     52|\n",
      "|  78352|     32|      4|     52|\n",
      "| 122804|     33|      5|     45|\n",
      "| 122804|     18|     36|     42|\n",
      "|  78352|     31|      5|     36|\n",
      "|  78352|     11|      4|     33|\n",
      "| 122804|     39|      7|     28|\n",
      "|  78352|     39|      7|     27|\n",
      "|  78352|     18|     36|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# User 1 and 2 (78352, 122804)\n",
    "spark.sql(\"\"\"\n",
    "    WITH group_marker AS (\n",
    "        SELECT user_id, CAST(x/50 AS INTEGER) AS x_group, CAST(y/50 AS INTEGER) AS y_group, x, y FROM data WHERE user_id IN (78352, 122804)\n",
    "    ), grouped_placements AS (\n",
    "        SELECT user_id, x_group, y_group, COUNT(1) AS actions FROM group_marker GROUP BY user_id, x_group, y_group\n",
    "    )\n",
    "       \n",
    "    SELECT user_id, x_group, y_group, actions FROM grouped_placements WHERE user_id IN (78352, 122804) ORDER BY actions DESC, x_group, y_group\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=========================================>              (11 + 4) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|x_group|y_group|actions|\n",
      "+-------+-------+-------+\n",
      "|     18|     37|    154|\n",
      "|     12|      5|    148|\n",
      "|     33|      4|    141|\n",
      "|     33|      5|    136|\n",
      "|     32|      4|    131|\n",
      "|     32|      5|    122|\n",
      "|     12|      4|    117|\n",
      "|     18|     36|     68|\n",
      "|     39|      7|     55|\n",
      "|     11|      4|     53|\n",
      "|     31|      5|     36|\n",
      "|     11|      5|     34|\n",
      "|     12|      9|     34|\n",
      "|     27|     28|     31|\n",
      "|     19|     37|     25|\n",
      "|     35|      4|     23|\n",
      "|     36|      9|     22|\n",
      "|     26|     39|     17|\n",
      "|     11|      9|     15|\n",
      "|     13|      9|     15|\n",
      "+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# User 1 and 2 (78352, 122804)\n",
    "spark.sql(\"\"\"\n",
    "    WITH group_marker AS (\n",
    "        SELECT user_id, CAST(x/50 AS INTEGER) AS x_group, CAST(y/50 AS INTEGER) AS y_group, x, y FROM data WHERE user_id IN (78352, 122804)\n",
    "    ), grouped_placements AS (\n",
    "        SELECT x_group, y_group, COUNT(1) AS actions FROM group_marker GROUP BY x_group, y_group\n",
    "    )\n",
    "       \n",
    "    SELECT x_group, y_group, actions FROM grouped_placements ORDER BY actions DESC, x_group, y_group\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_white = time_delta(\"2022-04-04 12:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Draw images at active spots\n",
    "draw_curr_img_at_group(8, 5, 50, before_white)\n",
    "draw_curr_img_at_group_from_users(8, 5, 50, [280238], before_white)\n",
    "draw_curr_img_at_group(12, 5, 50, before_white)\n",
    "draw_curr_img_at_group_from_users(12, 5, 50, [122804, 78352], before_white)\n",
    "draw_curr_img_at_group(33, 5, 50, before_white)\n",
    "draw_curr_img_at_group_from_users(33, 5, 50, [78352, 122804], before_white)\n",
    "draw_curr_img_at_group(18, 37, 50, before_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlarged images at similar blocks\n",
    "draw_curr_img(32*50, 4*50, 34*50-1, 6*50-1, before_white)\n",
    "draw_curr_img(7*50, 5*50, 9*50-1, 7*50-1, before_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Focus on user placements in specific region\n",
    "draw_curr_img_focus_users(392, 252, 433, 298, [280238], before_white)\n",
    "draw_curr_img_focus_users(1614, 212, 1688, 280, [122804, 78352], before_white)\n",
    "draw_curr_img_focus_users(892, 1830, 961, 1885, [122804, 78352], before_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
